# Segment Anything Model (SAM) ViT-B - Foundation Segmentation
# Source: https://segment-anything.com/

id: sam_vit_b
name: Segment Anything Model ViT-B
version: "1.0"
vendor: meta

architecture:
  type: instance_segmentation
  family: sam
  backbone: ViT-B/16
  neck: null
  head: Mask Decoder
  params_millions: 91.0
  flops_billions: 371.0
  layers: 12
  attention_layers: 12

input:
  shape: [1, 3, 1024, 1024]
  dtype: float32
  normalization: [0, 1]
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  channel_order: rgb

output:
  format: masks_iou_scores
  output_names: [masks, iou_predictions]

accuracy:
  sa1b:
    dataset: sa1b
    miou: 72.3
    methodology: official
    notes: "Zero-shot segmentation across domains"

variants:
  - name: fp32-pytorch
    dtype: fp32
    format: pytorch
    file_size_mb: 375.0
    accuracy_delta: 0.0
  - name: fp16-onnx
    dtype: fp16
    format: onnx
    file_size_mb: 190.0
    accuracy_delta: -0.1

memory:
  weights_mb: 375.0
  peak_activation_mb: 2000
  workspace_mb: 500

compatible_hardware:
  - nvidia_jetson_agx_orin_64gb
  - nvidia_jetson_agx_thor_128gb

suitable_for:
  - industrial_inspection
  - robotic_manipulation

optimization_notes:
  tensorrt: "ViT encoder dominates compute. Pre-compute embeddings for interactive use."
  note: "Foundation model - supports point, box, and mask prompts."

license: Apache-2.0
source_url: https://github.com/facebookresearch/segment-anything
paper_url: https://arxiv.org/abs/2304.02643
weights_url: https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth
last_updated: "2025-12-31"
